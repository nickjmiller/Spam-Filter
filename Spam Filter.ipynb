{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier on Fake Reviews\n",
    "How to create and train a classifier to spot fake reviews on Yelp, using supervised learning and the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sqlite3\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "We are using a dataset of Yelp reviews that is stored in a SQL database. These reviews are flagged as either 'fake' or 'real', and there is some additional information about each review. The first step is to grab these reviews and put them in a dataframe. We'll be using pandas, which feels very familiar coming from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('yelpHotelData.db')\n",
    "query = 'SELECT reviewContent, rating, usefulCount, coolCount, funnyCount FROM review WHERE flagged = \"Y\" OR flagged = \"YR\"'\n",
    "fake = pd.read_sql(query, conn)\n",
    "query = 'SELECT reviewContent, rating, usefulCount, coolCount, funnyCount FROM review WHERE flagged = \"N\" OR flagged = \"NR\"'\n",
    "real = pd.read_sql(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have over 700 thousand reviews, that's a lot to process. Instead we will randomly sample 5000 each from both classes, and combine them in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake = fake.sample(5000)\n",
    "fake['tag'] = 'fake'\n",
    "real = real.sample(5000)\n",
    "real['tag'] = 'real'\n",
    "df = pd.concat([fake,real])\n",
    "df = df.iloc[np.random.permutation(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a single dataframe, we need to use the nltk to \"tokenize\" the words in the reviews. This will make each word easy to access, and allow us to get a frequency distribution accross all of the reviews to find the 2000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['reviewContent'] = df.apply(lambda row: nltk.word_tokenize(row['reviewContent']), axis=1)\n",
    "all_words = nltk.FreqDist(word.lower() for row in df['reviewContent'] for word in row)\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_features list contains the 2000 most common words in the reviews. We will use these words as binary features ( true if in a review, false otherwise). Now these data structures are very expensive, let's do some desperate cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(all_words)\n",
    "del(real)\n",
    "del(fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Feature Extractor\n",
    "We have our reviews in a nice dataframe, but now we need to start grabbing features for each review. We'll create a function that can features from each row of the dataframe. We will collect the word features mentioned earlier, among a few other features that seem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(doc):\n",
    "    document_words = set(doc['reviewContent'])\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains ' + word] = (word in document_words)\n",
    "    features.update(\n",
    "    {'rating': doc['rating'], 'useful': doc['usefulCount'], 'cool': doc['coolCount'], 'funny': doc['funnyCount'], 'length': len(doc['reviewContent'])})\n",
    "    return [features,doc['tag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function, we can easily apply it to the dataframe and create training and test sets. Our training set will be 80% of our data, and the test will be the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = df.apply(document_features, axis = 1)\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "del(word_features)\n",
    "del(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Now to train a learner on this data. We will use nltk's built-in naive bayes classifier first, since it is quick and easy to implement. We will simply train it and test it, and print the accuracy, along with what the classifier identifies as the most useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574\n",
      "Most Informative Features\n",
      "          contains pairs = True             real : fake   =      7.0 : 1.0\n",
      "                  length = 210              real : fake   =      6.6 : 1.0\n",
      "       contains estimate = True             real : fake   =      6.3 : 1.0\n",
      "      contains operation = True             real : fake   =      5.4 : 1.0\n",
      "      contains saltiness = True             fake : real   =      5.0 : 1.0\n",
      "                  length = 320              fake : real   =      5.0 : 1.0\n",
      "                  length = 282              real : fake   =      5.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scikit-Learn Classifiers\n",
    "Now let's try some other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier:\n",
      "0.527\n",
      "Adaboost Classifier:\n",
      "0.594\n",
      "Neural Network Classifier:\n",
      "0.524\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "svmClass = SklearnClassifier(SVC(C = .7)).train(train_set)\n",
    "print(\"SVM Classifier:\")\n",
    "print(nltk.classify.accuracy(svmClass, test_set))\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaClass = SklearnClassifier(AdaBoostClassifier()).train(train_set)\n",
    "print(\"Adaboost Classifier:\")\n",
    "print(nltk.classify.accuracy(adaClass, test_set))\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nnClass = SklearnClassifier(MLPClassifier()).train(train_set)\n",
    "print(\"Neural Network Classifier:\")\n",
    "print(nltk.classify.accuracy(nnClass, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
