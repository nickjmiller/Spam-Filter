{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier on Fake Reviews\n",
    "How to create and train a classifier to spot fake reviews on Yelp, using supervised learning and the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sqlite3\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "We are using a dataset of Yelp reviews that is stored in a SQL database. These reviews are flagged as either 'fake' or 'real', and there is some additional information about each review. The first step is to grab these reviews and put them in a dataframe. We'll be using pandas, which feels very familiar coming from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('yelpHotelData.db')\n",
    "query = 'SELECT reviewContent, rating, usefulCount, coolCount, funnyCount FROM review WHERE flagged = \"Y\"'\n",
    "fake = pd.read_sql(query, conn)\n",
    "query = 'SELECT reviewContent, rating, usefulCount, coolCount, funnyCount FROM review WHERE flagged = \"N\"'\n",
    "real = pd.read_sql(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the database has over 700,000 reviews, we are going to focus on a certain subset of them. We will focus on hotel reviews, and I'll explain why this is important when we get to the feature extractor. But for now, we need to make sure we have a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake = fake.sample(750)\n",
    "fake['tag'] = 'fake'\n",
    "real = real.sample(750)\n",
    "real['tag'] = 'real'\n",
    "df = pd.concat([fake,real])\n",
    "df = df.iloc[np.random.permutation(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a single dataframe, we need to use the nltk to \"tokenize\" the words in the reviews. This will make each word easy to access, and allow us to get a frequency distribution accross all of the reviews to find the 2000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['reviewContent'] = df.apply(lambda row: nltk.word_tokenize(row['reviewContent']), axis=1)\n",
    "all_words = nltk.FreqDist(word.lower() for row in df['reviewContent'] for word in row)\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_features list contains the 2000 most common words in the reviews. We will use these words as binary features ( true if in a review, false otherwise). This is why we need to focus on one type of review. Extracting these word features tell us the most obvious differences between reviews. It would be incredibly hard to determine a fake versus real review, when the most obvious differences would be between reviews about hotels, restaurants, etc.\n",
    "\n",
    "We can also pull the most common \"bigrams\", which is essentially the most common pairs of words (e.g. \"i am\", \"this house\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['bigrams'] = df.apply(lambda row: list(nltk.bigrams(row['reviewContent'])), axis=1)\n",
    "bigrams = nltk.FreqDist(word[0].lower() +\" \"+ word[1].lower() for row in df['bigrams'] for word in row)\n",
    "bigrams = list(bigrams)[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is very memory intensive, so we'll try to delete unused data structures as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(all_words)\n",
    "del(real)\n",
    "del(fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Feature Extractor\n",
    "We have our reviews in a nice dataframe, but now we need to start grabbing features for each review. We'll create a function that can features from each row of the dataframe. We will collect the word features mentioned earlier, among a few other features that seem useful. There is a lot to unpack here, but essentially we are grabbing the 2000 word features, 500 bigram features, the rating, useful count, etc. from the review, and whether or not they say \"me\" or \"I\" a lot. This combination of features seems to get consistently high accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(doc):\n",
    "    document_words = set(doc['reviewContent'])\n",
    "    features = {}\n",
    "    # Grabbing the bigrams\n",
    "    bigSet = []\n",
    "    for word in doc['bigrams']:\n",
    "        bigSet.append(word[0].lower() + \" \" +word[1].lower())\n",
    "    bigSet = set(bigSet)\n",
    "    for word in bigrams:\n",
    "        features['bigram: ' + word] = (word in bigSet)\n",
    "\n",
    "    # Counting the pronoun usage\n",
    "    meCount = 0\n",
    "    for word in doc['reviewContent']:\n",
    "        if (word.lower() == 'i' or word.lower() == 'me'):\n",
    "            meCount += 1\n",
    "    me = False\n",
    "    if (meCount > 5):\n",
    "        me = True\n",
    "\n",
    "    for word in word_features:\n",
    "        features['contains ' + word] = (word in document_words)\n",
    "    features.update(\n",
    "    {'rating': doc['rating'], 'useful': doc['usefulCount'], 'cool': doc['coolCount'], 'funny': doc['funnyCount'], 'meCount': me})\n",
    "    return [features,doc['tag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function, we can easily apply it to the dataframe and create training and test sets. Our training set will be 80% of our data, and the test will be the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = df.apply(document_features, axis = 1)\n",
    "train_set, test_set = featuresets[300:], featuresets[:300]\n",
    "del(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "Now to train a learner on this data. We will use nltk's built-in naive bayes classifier first, since it is quick and easy to implement. We will simply train it and test it, and print the accuracy, along with what the classifier identifies as the most useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set)*100)\n",
    "classifier.show_most_informative_features(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scikit-Learn Classifiers\n",
    "Now let's try some other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "svmClass = SklearnClassifier(SVC()).train(train_set)\n",
    "print(\"SVM Classifier:\")\n",
    "print(nltk.classify.accuracy(svmClass, test_set)*100)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaClass = SklearnClassifier(AdaBoostClassifier()).train(train_set)\n",
    "print(\"Adaboost Classifier:\")\n",
    "print(nltk.classify.accuracy(adaClass, test_set)*100)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nnClass = SklearnClassifier(MLPClassifier()).train(train_set)\n",
    "print(\"Neural Network Classifier:\")\n",
    "print(nltk.classify.accuracy(nnClass, test_set)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results\n",
    "The results are often consistent, Adaboost is almost always the best algorithm, with usually 75%+ accuracy. The SVM is consistently the worst, sometimes approaching 50%. The SVM is using the default settings and does not play nicely with the lexical features. Naive bayes and the Neural Network classifier are generally neck and neck. Naive bayes tends to linger around 70% constantly, but the neural network is less consistent, and ranges from 65-75% accuracy.\n",
    "\n",
    "## Optimization\n",
    "Scikit Learn has a lot of built in functions for cross-validation and finding ideal parameters. The issue is that the dataset is formatted to be used by nltk, which is why we needed the wrappers in the previous examples. If we extract our features separately, then convert them into numbers, we can start implementing these functions and see how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_features(doc):\n",
    "    \"\"\"This function returns the features without the tags\"\"\"\n",
    "    document_words = set(doc['reviewContent'])\n",
    "    features = {}\n",
    "    # Grabbing the bigrams\n",
    "    bigSet = []\n",
    "    for word in doc['bigrams']:\n",
    "        bigSet.append(word[0].lower() + \" \" +word[1].lower())\n",
    "    bigSet = set(bigSet)\n",
    "    for word in bigrams:\n",
    "        features['bigram: ' + word] = (word in bigSet)\n",
    "\n",
    "    # Counting the pronoun usage\n",
    "    meCount = 0\n",
    "    for word in doc['reviewContent']:\n",
    "        if (word.lower() == 'i' or word.lower() == 'me'):\n",
    "            meCount += 1\n",
    "    me = False\n",
    "    if (meCount > 5):\n",
    "        me = True\n",
    "\n",
    "    for word in word_features:\n",
    "        features['contains ' + word] = (word in document_words)\n",
    "    features.update(\n",
    "    {'rating': doc['rating'], 'useful': doc['usefulCount'], 'cool': doc['coolCount'], 'funny': doc['funnyCount'], 'meCount': me})\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the feature extractor to convert these features into numbers, and use grid search to find the best parameters for the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'kernel':('linear','poly', 'rbf'), 'C':[1, 10]}\n",
    "vec = DictVectorizer()\n",
    "trainTags, testTags = df[250:],df[:250]\n",
    "svmSet = df.apply(svm_features, axis = 1)\n",
    "svmSet = vec.fit_transform(svmSet).toarray()\n",
    "svmTest, svmTrain = svmSet[:250],svmSet[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr = SVC()\n",
    "svmClass = GridSearchCV(svr, parameters)\n",
    "svmClass.fit(svmTrain,trainTags['tag'])\n",
    "print(\"SVM with Grid Search Cross-Validation: \")\n",
    "print(svmClass.score(svmTest,testTags['tag'])*100)\n",
    "print(svmClass.get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "Clearly preprocessing for SVM had a huge impact. However, AdaBoost generally seems the most reliable for this kind of dataset. While it performs well, it's still not quite strong enough to be used in practice. In my own testing I've found that the false positive and negative rate is usually about equal, so this classifier doesn't seem to lean one way or the other. There is more that can be done, the word tokenization can be optimized, and more advanced NLP tools could be used (such as part of speech tagging). Being able to see user information, such as posting habits or rating habits, could also greatly uplift the accuracy of the classifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
